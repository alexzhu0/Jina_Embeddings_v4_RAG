#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ”¿åºœå·¥ä½œæŠ¥å‘Šæ–‡æ¡£å¤„ç†å™¨
è´Ÿè´£Wordæ–‡æ¡£è§£æã€æ–‡æœ¬æå–ã€çœä»½è¯†åˆ«å’Œåˆ†æ®µå¤„ç†
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import logging

# å¯¼å…¥æ–‡æ¡£å¤„ç†åº“
from docx import Document
import jieba
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç»“æ„"""
    id: str
    province: str
    content: str
    chunk_type: str  # 'title', 'content', 'target', 'summary'
    metadata: Dict
    char_count: int
    source: str = ""  # æ·»åŠ sourceå±æ€§
    start_pos: int = 0  # æ·»åŠ start_poså±æ€§
    end_pos: int = 0  # æ·»åŠ end_poså±æ€§
    chunk_id: int = 0  # æ·»åŠ chunk_idå±æ€§
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)

class GovernmentReportProcessor:
    """æ”¿åºœå·¥ä½œæŠ¥å‘Šå¤„ç†å™¨"""
    
    def __init__(self, raw_documents_path: str, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            raw_documents_path: åŸå§‹æ–‡æ¡£è·¯å¾„
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        self.raw_documents_path = Path(raw_documents_path)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # çœä»½è¯†åˆ«æ¨¡å¼
        self.province_patterns = {
            "åŒ—äº¬": ["åŒ—äº¬", "äº¬"],
            "å¤©æ´¥": ["å¤©æ´¥", "æ´¥"],
            "æ²³åŒ—": ["æ²³åŒ—", "å†€"],
            "å±±è¥¿": ["å±±è¥¿", "æ™‹"],
            "å†…è’™å¤": ["å†…è’™å¤", "å†…è’™", "è’™"],
            "è¾½å®": ["è¾½å®", "è¾½"],
            "å‰æ—": ["å‰æ—", "å‰"],
            "é»‘é¾™æ±Ÿ": ["é»‘é¾™æ±Ÿ", "é»‘"],
            "ä¸Šæµ·": ["ä¸Šæµ·", "æ²ª"],
            "æ±Ÿè‹": ["æ±Ÿè‹", "è‹"],
            "æµ™æ±Ÿ": ["æµ™æ±Ÿ", "æµ™"],
            "å®‰å¾½": ["å®‰å¾½", "çš–"],
            "ç¦å»º": ["ç¦å»º", "é—½"],
            "æ±Ÿè¥¿": ["æ±Ÿè¥¿", "èµ£"],
            "å±±ä¸œ": ["å±±ä¸œ", "é²"],
            "æ²³å—": ["æ²³å—", "è±«"],
            "æ¹–åŒ—": ["æ¹–åŒ—", "é„‚"],
            "æ¹–å—": ["æ¹–å—", "æ¹˜"],
            "å¹¿ä¸œ": ["å¹¿ä¸œ", "ç²¤"],
            "å¹¿è¥¿": ["å¹¿è¥¿", "æ¡‚"],
            "æµ·å—": ["æµ·å—", "ç¼"],
            "é‡åº†": ["é‡åº†", "æ¸"],
            "å››å·": ["å››å·", "å·", "èœ€"],
            "è´µå·": ["è´µå·", "é»”", "è´µ"],
            "äº‘å—": ["äº‘å—", "æ»‡", "äº‘"],
            "è¥¿è—": ["è¥¿è—", "è—"],
            "é™•è¥¿": ["é™•è¥¿", "é™•", "ç§¦"],
            "ç”˜è‚ƒ": ["ç”˜è‚ƒ", "ç”˜", "é™‡"],
            "é’æµ·": ["é’æµ·", "é’"],
            "å®å¤": ["å®å¤", "å®"],
            "æ–°ç–†": ["æ–°ç–†", "æ–°"]
        }
        
        # å…³é”®è¯æ¨¡å¼ï¼ˆç”¨äºè¯†åˆ«é‡è¦å†…å®¹ï¼‰
        self.target_keywords = [
            "ä¸»è¦ç›®æ ‡", "å·¥ä½œç›®æ ‡", "å‘å±•ç›®æ ‡", "é‡ç‚¹ä»»åŠ¡", "ä¸»è¦ä»»åŠ¡",
            "å·¥ä½œé‡ç‚¹", "é‡ç‚¹å·¥ä½œ", "é‡ç‚¹é¡¹ç›®", "é‡å¤§é¡¹ç›®", "é‡å¤§å·¥ç¨‹",
            "äº§ä¸šå‘å±•", "ç»æµå‘å±•", "ç¤¾ä¼šå‘å±•", "æ°‘ç”Ÿæ”¹å–„", "ç”Ÿæ€ç¯å¢ƒ"
        ]
        
        logger.info(f"ğŸ“ æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‚ æ–‡æ¡£è·¯å¾„: {self.raw_documents_path}")
        logger.info(f"ğŸ“ åˆ†å—å¤§å°: {chunk_size}")
    
    def extract_province_from_filename(self, filename: str) -> Optional[str]:
        """
        ä»æ–‡ä»¶åä¸­æå–çœä»½ä¿¡æ¯
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            Optional[str]: çœä»½åç§°
        """
        for province, patterns in self.province_patterns.items():
            for pattern in patterns:
                if pattern in filename:
                    return province
        return None
    
    def extract_province_from_content(self, content: str) -> Optional[str]:
        """
        ä»å†…å®¹ä¸­æå–çœä»½ä¿¡æ¯
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            
        Returns:
            Optional[str]: çœä»½åç§°
        """
        # åœ¨æ–‡æ¡£å¼€å¤´éƒ¨åˆ†æœç´¢çœä»½ä¿¡æ¯
        header_content = content[:1000]  # åªæ£€æŸ¥å‰1000å­—ç¬¦
        
        for province, patterns in self.province_patterns.items():
            for pattern in patterns:
                if pattern in header_content:
                    return province
        return None
    
    def read_docx_file(self, file_path: Path) -> Tuple[str, Dict]:
        """
        è¯»å–Wordæ–‡æ¡£å†…å®¹
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            
        Returns:
            Tuple[str, Dict]: (æ–‡æ¡£å†…å®¹, å…ƒæ•°æ®)
        """
        try:
            doc = Document(file_path)
            
            # æå–æ–‡æœ¬å†…å®¹
            paragraphs = []
            for paragraph in doc.paragraphs:
                text = paragraph.text.strip()
                if text:  # è·³è¿‡ç©ºæ®µè½
                    paragraphs.append(text)
            
            content = "\n".join(paragraphs)
            
            # æå–å…ƒæ•°æ®
            metadata = {
                "filename": file_path.name,
                "file_size": file_path.stat().st_size,
                "paragraph_count": len(paragraphs),
                "char_count": len(content)
            }
            
            logger.debug(f"ğŸ“„ è¯»å–æ–‡æ¡£: {file_path.name} ({metadata['char_count']} å­—ç¬¦)")
            
            return content, metadata
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡æ¡£å¤±è´¥ {file_path.name}: {str(e)}")
            return "", {}
    
    def identify_content_type(self, text: str) -> str:
        """
        è¯†åˆ«å†…å®¹ç±»å‹
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            str: å†…å®¹ç±»å‹
        """
        text_lower = text.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
        for keyword in self.target_keywords:
            if keyword in text:
                return "target"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜ï¼ˆé€šå¸¸è¾ƒçŸ­ä¸”åŒ…å«ç‰¹å®šæ ¼å¼ï¼‰
        if len(text) < 100 and any(char in text for char in "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å"):
            return "title"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‘˜è¦ï¼ˆé€šå¸¸åœ¨æ–‡æ¡£å¼€å¤´ï¼‰
        if "æ‘˜è¦" in text or "æ¦‚è¿°" in text or "æ€»ä½“" in text:
            return "summary"
        
        return "content"
    
    def split_text_into_chunks(self, text: str, province: str, metadata: Dict) -> List[DocumentChunk]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºå—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            province: çœä»½åç§°
            metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            List[DocumentChunk]: æ–‡æ¡£å—åˆ—è¡¨
        """
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n')
        
        current_chunk = ""
        chunk_id = 0
        current_pos = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # æ£€æŸ¥æ·»åŠ è¿™ä¸ªæ®µè½æ˜¯å¦ä¼šè¶…è¿‡å—å¤§å°é™åˆ¶
            if len(current_chunk) + len(paragraph) > self.chunk_size and current_chunk:
                # åˆ›å»ºå½“å‰å—
                chunk_content = current_chunk.strip()
                chunk = DocumentChunk(
                    id=f"{province}_{chunk_id:03d}",
                    province=province,
                    content=chunk_content,
                    chunk_type=self.identify_content_type(chunk_content),
                    metadata={
                        **metadata,
                        "chunk_index": chunk_id,
                        "paragraph_start": max(0, len(current_chunk) - self.chunk_overlap)
                    },
                    char_count=len(chunk_content),
                    source=metadata.get("filename", ""),
                    start_pos=current_pos,
                    end_pos=current_pos + len(chunk_content),
                    chunk_id=chunk_id
                )
                chunks.append(chunk)
                
                # æ›´æ–°ä½ç½®
                current_pos += len(chunk_content)
                
                # å¼€å§‹æ–°å—ï¼ˆä¿ç•™é‡å éƒ¨åˆ†ï¼‰
                if self.chunk_overlap > 0 and len(current_chunk) > self.chunk_overlap:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + "\n" + paragraph
                    current_pos -= len(overlap_text)  # å›é€€é‡å éƒ¨åˆ†çš„ä½ç½®
                else:
                    current_chunk = paragraph
                
                chunk_id += 1
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                if current_chunk:
                    current_chunk += "\n" + paragraph
                else:
                    current_chunk = paragraph
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunk_content = current_chunk.strip()
            chunk = DocumentChunk(
                id=f"{province}_{chunk_id:03d}",
                province=province,
                content=chunk_content,
                chunk_type=self.identify_content_type(chunk_content),
                metadata={
                    **metadata,
                    "chunk_index": chunk_id,
                    "is_last_chunk": True
                },
                char_count=len(chunk_content),
                source=metadata.get("filename", ""),
                start_pos=current_pos,
                end_pos=current_pos + len(chunk_content),
                chunk_id=chunk_id
            )
            chunks.append(chunk)
        
        return chunks
    
    def process_single_document(self, file_path: Path) -> List[DocumentChunk]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            
        Returns:
            List[DocumentChunk]: æ–‡æ¡£å—åˆ—è¡¨
        """
        logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {file_path.name}")
        
        # è¯»å–æ–‡æ¡£å†…å®¹
        content, metadata = self.read_docx_file(file_path)
        
        if not content:
            logger.warning(f"âš ï¸ æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path.name}")
            return []
        
        # è¯†åˆ«çœä»½
        province = self.extract_province_from_filename(file_path.name)
        if not province:
            province = self.extract_province_from_content(content)
        
        if not province:
            logger.warning(f"âš ï¸ æ— æ³•è¯†åˆ«çœä»½: {file_path.name}")
            province = "æœªçŸ¥"
        
        # åˆ†å‰²æ–‡æœ¬
        chunks = self.split_text_into_chunks(content, province, metadata)
        
        logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {province} ({len(chunks)} ä¸ªå—)")
        
        return chunks
    
    def process_all_documents(self) -> List[DocumentChunk]:
        """
        å¤„ç†æ‰€æœ‰æ–‡æ¡£
        
        Returns:
            List[DocumentChunk]: æ‰€æœ‰æ–‡æ¡£å—åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ‰€æœ‰æ–‡æ¡£...")
        
        if not self.raw_documents_path.exists():
            logger.error(f"âŒ æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨: {self.raw_documents_path}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰Wordæ–‡æ¡£
        doc_files = list(self.raw_documents_path.glob("*.docx"))
        doc_files.extend(self.raw_documents_path.glob("*.doc"))
        
        if not doc_files:
            logger.error(f"âŒ æœªæ‰¾åˆ°Wordæ–‡æ¡£: {self.raw_documents_path}")
            return []
        
        logger.info(f"ğŸ“š æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£")
        
        all_chunks = []
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        for file_path in tqdm(doc_files, desc="ğŸ“– å¤„ç†æ–‡æ¡£"):
            chunks = self.process_single_document(file_path)
            all_chunks.extend(chunks)
        
        # ç»Ÿè®¡ä¿¡æ¯
        province_stats = {}
        for chunk in all_chunks:
            province = chunk.province
            if province not in province_stats:
                province_stats[province] = 0
            province_stats[province] += 1
        
        logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»å—æ•°: {len(all_chunks)}")
        logger.info(f"ğŸ—ºï¸ çœä»½ç»Ÿè®¡: {province_stats}")
        
        return all_chunks
    
    def save_processed_data(self, chunks: List[DocumentChunk], output_path: Path) -> bool:
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            chunks: æ–‡æ¡£å—åˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            json_file = output_path / "processed_chunks.json"
            
            chunks_data = [chunk.to_dict() for chunk in chunks]
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(chunks_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = output_path / "processing_stats.json"
            
            province_stats = {}
            type_stats = {}
            
            for chunk in chunks:
                # çœä»½ç»Ÿè®¡
                province = chunk.province
                if province not in province_stats:
                    province_stats[province] = {"count": 0, "total_chars": 0}
                province_stats[province]["count"] += 1
                province_stats[province]["total_chars"] += chunk.char_count
                
                # ç±»å‹ç»Ÿè®¡
                chunk_type = chunk.chunk_type
                if chunk_type not in type_stats:
                    type_stats[chunk_type] = 0
                type_stats[chunk_type] += 1
            
            stats = {
                "total_chunks": len(chunks),
                "total_documents": len(set(chunk.province for chunk in chunks)),
                "province_stats": province_stats,
                "type_stats": type_stats,
                "processing_config": {
                    "chunk_size": self.chunk_size,
                    "chunk_overlap": self.chunk_overlap
                }
            }
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ æ•°æ®ä¿å­˜æˆåŠŸ: {json_file}")
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿å­˜æˆåŠŸ: {stats_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def load_processed_data(self, data_path: Path) -> List[DocumentChunk]:
        """
        åŠ è½½å·²å¤„ç†çš„æ•°æ®
        
        Args:
            data_path: æ•°æ®è·¯å¾„
            
        Returns:
            List[DocumentChunk]: æ–‡æ¡£å—åˆ—è¡¨
        """
        try:
            json_file = data_path / "processed_chunks.json"
            
            if not json_file.exists():
                logger.warning(f"âš ï¸ å¤„ç†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
                return []
            
            with open(json_file, 'r', encoding='utf-8') as f:
                chunks_data = json.load(f)
            
            chunks = []
            for chunk_data in chunks_data:
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å±æ€§éƒ½å­˜åœ¨ï¼Œä¸ºç¼ºå¤±çš„å±æ€§æä¾›é»˜è®¤å€¼
                chunk_data.setdefault('source', chunk_data.get('metadata', {}).get('filename', ''))
                chunk_data.setdefault('start_pos', 0)
                chunk_data.setdefault('end_pos', chunk_data.get('char_count', 0))
                chunk_data.setdefault('chunk_id', chunk_data.get('metadata', {}).get('chunk_index', 0))
                
                chunk = DocumentChunk(**chunk_data)
                chunks.append(chunk)
            
            logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®æˆåŠŸ: {len(chunks)} ä¸ªå—")
            
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            return []

 