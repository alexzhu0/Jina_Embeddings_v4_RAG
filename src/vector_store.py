#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å‘é‡å­˜å‚¨ç®¡ç†å™¨
è´Ÿè´£æ–‡æ¡£å‘é‡åŒ–ã€å­˜å‚¨å’Œæ£€ç´¢
"""

import os
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
import faiss
from tqdm import tqdm

from data_processor import DocumentChunk
from embedding_manager import get_embedding_manager

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorStore:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, store_path: str, embedding_dim: int = 1024):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨
        
        Args:
            store_path: å­˜å‚¨è·¯å¾„
            embedding_dim: å‘é‡ç»´åº¦
        """
        self.store_path = Path(store_path)
        self.embedding_dim = embedding_dim
        
        # FAISSç´¢å¼•
        self.index = None
        self.chunks = []  # å­˜å‚¨æ–‡æ¡£å—ä¿¡æ¯
        self.chunk_embeddings = None
        
        # æ–‡ä»¶è·¯å¾„
        self.index_file = self.store_path / "faiss_index.bin"
        self.chunks_file = self.store_path / "chunks_metadata.pkl"
        self.embeddings_file = self.store_path / "embeddings.npy"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.store_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ—„ï¸ å‘é‡å­˜å‚¨åˆå§‹åŒ–")
        logger.info(f"ğŸ“ å­˜å‚¨è·¯å¾„: {self.store_path}")
        logger.info(f"ğŸ“ å‘é‡ç»´åº¦: {embedding_dim}")
    
    def _create_faiss_index(self, dimension: int) -> faiss.Index:
        """
        åˆ›å»ºFAISSç´¢å¼•
        
        Args:
            dimension: å‘é‡ç»´åº¦
            
        Returns:
            faiss.Index: FAISSç´¢å¼•
        """
        # ä½¿ç”¨L2è·ç¦»çš„å¹³é¢ç´¢å¼•ï¼ˆé€‚åˆä¸­å°è§„æ¨¡æ•°æ®ï¼‰
        index = faiss.IndexFlatL2(dimension)
        
        # å¦‚æœæ•°æ®é‡å¤§ï¼Œå¯ä»¥ä½¿ç”¨IVFç´¢å¼•
        # nlist = 100  # èšç±»ä¸­å¿ƒæ•°é‡
        # quantizer = faiss.IndexFlatL2(dimension)
        # index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
        
        logger.info(f"ğŸ“Š åˆ›å»ºFAISSç´¢å¼•: {type(index).__name__}")
        return index
    
    def build_index(self, chunks: List[DocumentChunk], batch_size: int = 32, embedding_manager=None) -> bool:
        """
        æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            chunks: æ–‡æ¡£å—åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            embedding_manager: embeddingç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            bool: æ˜¯å¦æ„å»ºæˆåŠŸ
        """
        logger.info(f"ğŸ”¨ å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
        logger.info(f"ğŸ“š æ–‡æ¡£å—æ•°é‡: {len(chunks)}")
        
        if not chunks:
            logger.error("âŒ æ²¡æœ‰æ–‡æ¡£å—å¯ä»¥ç´¢å¼•")
            return False
        
        try:
            # è·å–embeddingç®¡ç†å™¨ - ä½¿ç”¨SDPAä¼˜åŒ–
            if embedding_manager is None:
                embedding_manager = get_embedding_manager(attn_implementation="sdpa")
            
            if not embedding_manager.is_model_loaded():
                logger.error("âŒ Embeddingæ¨¡å‹æœªåŠ è½½")
                return False
            
            # æå–æ–‡æœ¬å†…å®¹
            texts = [chunk.content for chunk in chunks]
            
            # æ‰¹é‡ç¼–ç æ–‡æœ¬
            logger.info("ğŸ”„ æ­£åœ¨ç¼–ç æ–‡æœ¬...")
            embeddings = embedding_manager.encode_texts(
                texts, 
                batch_size=batch_size, 
                show_progress=True
            )
            
            if len(embeddings) == 0:
                logger.error("âŒ æ–‡æœ¬ç¼–ç å¤±è´¥")
                return False
            
            # æ›´æ–°å‘é‡ç»´åº¦
            self.embedding_dim = embeddings.shape[1]
            logger.info(f"ğŸ“ å®é™…å‘é‡ç»´åº¦: {self.embedding_dim}")
            
            # åˆ›å»ºFAISSç´¢å¼•
            self.index = self._create_faiss_index(self.embedding_dim)
            
            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            logger.info("ğŸ“Š æ·»åŠ å‘é‡åˆ°ç´¢å¼•...")
            self.index.add(embeddings.astype(np.float32))
            
            # ä¿å­˜æ•°æ®
            self.chunks = chunks
            self.chunk_embeddings = embeddings
            
            logger.info(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ!")
            logger.info(f"ğŸ“Š ç´¢å¼•å¤§å°: {self.index.ntotal}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def save_index(self) -> bool:
        """
        ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
        
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            if self.index is None:
                logger.error("âŒ æ²¡æœ‰å¯ä¿å­˜çš„ç´¢å¼•")
                return False
            
            logger.info("ğŸ’¾ ä¿å­˜å‘é‡ç´¢å¼•...")
            
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.index, str(self.index_file))
            
            # ä¿å­˜æ–‡æ¡£å—å…ƒæ•°æ®
            with open(self.chunks_file, 'wb') as f:
                pickle.dump(self.chunks, f)
            
            # ä¿å­˜embeddings
            if self.chunk_embeddings is not None:
                np.save(self.embeddings_file, self.chunk_embeddings)
            
            logger.info(f"âœ… ç´¢å¼•ä¿å­˜æˆåŠŸ!")
            logger.info(f"ğŸ“„ ç´¢å¼•æ–‡ä»¶: {self.index_file}")
            logger.info(f"ğŸ“„ å…ƒæ•°æ®æ–‡ä»¶: {self.chunks_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def load_index(self) -> bool:
        """
        ä»ç£ç›˜åŠ è½½ç´¢å¼•
        
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            if not self.index_file.exists():
                logger.warning(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_file}")
                return False
            
            logger.info("ğŸ“‚ åŠ è½½å‘é‡ç´¢å¼•...")
            
            # åŠ è½½FAISSç´¢å¼•
            self.index = faiss.read_index(str(self.index_file))
            
            # åŠ è½½æ–‡æ¡£å—å…ƒæ•°æ®
            if self.chunks_file.exists():
                with open(self.chunks_file, 'rb') as f:
                    self.chunks = pickle.load(f)
            
            # åŠ è½½embeddings
            if self.embeddings_file.exists():
                self.chunk_embeddings = np.load(self.embeddings_file)
            
            # æ›´æ–°ç»´åº¦ä¿¡æ¯
            if self.index:
                self.embedding_dim = self.index.d
            
            logger.info(f"âœ… ç´¢å¼•åŠ è½½æˆåŠŸ!")
            logger.info(f"ğŸ“Š ç´¢å¼•å¤§å°: {self.index.ntotal}")
            logger.info(f"ğŸ“š æ–‡æ¡£å—æ•°é‡: {len(self.chunks)}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def search(self, query: str, top_k: int = 10, 
               province_filter: Optional[str] = None,
               chunk_type_filter: Optional[str] = None) -> List[Tuple[DocumentChunk, float]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            province_filter: çœä»½è¿‡æ»¤
            chunk_type_filter: å—ç±»å‹è¿‡æ»¤
            
        Returns:
            List[Tuple[DocumentChunk, float]]: (æ–‡æ¡£å—, ç›¸ä¼¼åº¦åˆ†æ•°)åˆ—è¡¨
        """
        if self.index is None:
            logger.error("âŒ ç´¢å¼•æœªåŠ è½½")
            return []
        
        try:
            # ç¼–ç æŸ¥è¯¢æ–‡æœ¬ - ä½¿ç”¨SDPAä¼˜åŒ–
            embedding_manager = get_embedding_manager(attn_implementation="sdpa")
            query_embedding = embedding_manager.encode_texts([query], show_progress=False)
            
            if len(query_embedding) == 0:
                logger.error("âŒ æŸ¥è¯¢ç¼–ç å¤±è´¥")
                return []
            
            # æœç´¢ç›¸ä¼¼å‘é‡ - æ”¯æŒå¤§é‡æ£€ç´¢
            search_k = min(max(top_k * 4, 200), self.index.ntotal)  # è‡³å°‘æœç´¢200ä¸ªç»“æœï¼Œæœ€å¤š4å€
            scores, indices = self.index.search(
                query_embedding.astype(np.float32), 
                search_k
            )
            
            results = []
            
            for score, idx in zip(scores[0], indices[0]):
                if idx >= len(self.chunks):
                    continue
                
                chunk = self.chunks[idx]
                
                # åº”ç”¨è¿‡æ»¤å™¨
                if province_filter and chunk.province != province_filter:
                    continue
                
                if chunk_type_filter and chunk.chunk_type != chunk_type_filter:
                    continue
                
                # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (L2è·ç¦»è½¬ä½™å¼¦ç›¸ä¼¼åº¦è¿‘ä¼¼)
                similarity = 1.0 / (1.0 + score)
                
                results.append((chunk, similarity))
                
                if len(results) >= top_k:
                    break
            
            logger.debug(f"ğŸ” æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:50]}...', ç»“æœæ•°={len(results)}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def get_chunks_by_province(self, province: str) -> List[DocumentChunk]:
        """
        è·å–æŒ‡å®šçœä»½çš„æ‰€æœ‰æ–‡æ¡£å—
        
        Args:
            province: çœä»½åç§°
            
        Returns:
            List[DocumentChunk]: æ–‡æ¡£å—åˆ—è¡¨
        """
        return [chunk for chunk in self.chunks if chunk.province == province]
    
    def get_chunks_by_type(self, chunk_type: str) -> List[DocumentChunk]:
        """
        è·å–æŒ‡å®šç±»å‹çš„æ‰€æœ‰æ–‡æ¡£å—
        
        Args:
            chunk_type: å—ç±»å‹
            
        Returns:
            List[DocumentChunk]: æ–‡æ¡£å—åˆ—è¡¨
        """
        return [chunk for chunk in self.chunks if chunk.chunk_type == chunk_type]
    
    def get_statistics(self) -> Dict:
        """
        è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.chunks:
            return {}
        
        # çœä»½ç»Ÿè®¡
        province_stats = {}
        type_stats = {}
        
        for chunk in self.chunks:
            # çœä»½ç»Ÿè®¡
            province = chunk.province
            if province not in province_stats:
                province_stats[province] = {"count": 0, "total_chars": 0}
            province_stats[province]["count"] += 1
            province_stats[province]["total_chars"] += chunk.char_count
            
            # ç±»å‹ç»Ÿè®¡
            chunk_type = chunk.chunk_type
            if chunk_type not in type_stats:
                type_stats[chunk_type] = 0
            type_stats[chunk_type] += 1
        
        return {
            "total_chunks": len(self.chunks),
            "total_provinces": len(province_stats),
            "index_size": self.index.ntotal if self.index else 0,
            "embedding_dimension": self.embedding_dim,
            "province_stats": province_stats,
            "type_stats": type_stats
        }
    
    def is_built(self) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²æ„å»º"""
        return self.index is not None and self.index.ntotal > 0

# å…¨å±€å‘é‡å­˜å‚¨å®ä¾‹
_vector_store = None

def get_vector_store() -> VectorStore:
    """è·å–å…¨å±€å‘é‡å­˜å‚¨å®ä¾‹ - ä½¿ç”¨SDPAä¼˜åŒ–"""
    global _vector_store
    if _vector_store is None:
        from config.config import DATA_PATHS
        
        # è·å–embeddingç»´åº¦ - ä½¿ç”¨SDPAä¼˜åŒ–
        embedding_manager = get_embedding_manager(attn_implementation="sdpa")
        embedding_dim = embedding_manager.get_embedding_dimension()
        
        _vector_store = VectorStore(
            store_path=str(DATA_PATHS["vector_store"]),
            embedding_dim=embedding_dim
        )
        
        # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
        _vector_store.load_index()
    
    return _vector_store

 