# FlashAttention2 Windowså®‰è£…æŒ‡å—

## ğŸ¯ RTX 3060 GPUæ”¯æŒFlashAttention2

æ‚¨çš„RTX 3060ï¼ˆAmpereæ¶æ„ï¼‰å®Œå…¨æ”¯æŒFlashAttention2ï¼è¿™å°†æ˜¾è‘—æå‡Jina Embeddings v4çš„æ€§èƒ½ã€‚

## ğŸ“‹ å‰ç½®æ¡ä»¶

âœ… **å·²æ»¡è¶³**ï¼š
- NVIDIA GeForce RTX 3060ï¼ˆAmpereæ¶æ„ï¼‰
- CUDA 12.1
- PyTorch 2.5.1+cu121
- Python 3.10

## ğŸš€ å®‰è£…æ–¹æ³•

### æ–¹æ³•1ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„Wheelæ–‡ä»¶ï¼ˆæ¨èï¼‰

1. **ä¸‹è½½é¢„ç¼–è¯‘çš„wheelæ–‡ä»¶**ï¼š
   
   è®¿é—®ä»¥ä¸‹ä»»ä¸€é“¾æ¥ä¸‹è½½é€‚åˆPython 3.10çš„wheelæ–‡ä»¶ï¼š
   
   - [GitHub - sunsetcoder](https://github.com/sunsetcoder/flash-attention-windows/blob/main/flash_attn-2.7.0.post2-cp310-cp310-win_amd64.whl)
   - [Hugging Face - lldacing](https://huggingface.co/lldacing/flash-attention-windows-wheel/tree/main)
   
   é€‰æ‹©æ–‡ä»¶ï¼š`flash_attn-2.7.0.post2-cp310-cp310-win_amd64.whl`

2. **å®‰è£…wheelæ–‡ä»¶**ï¼š
   ```bash
   # è¿›å…¥ä¸‹è½½ç›®å½•
   cd ä¸‹è½½ç›®å½•
   
   # å®‰è£…
   pip install flash_attn-2.7.0.post2-cp310-cp310-win_amd64.whl
   ```

### æ–¹æ³•2ï¼šå¯ç”¨Windowsé•¿è·¯å¾„æ”¯æŒåç¼–è¯‘

1. **ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡ŒPowerShell**
2. **æ‰§è¡Œå¯ç”¨é•¿è·¯å¾„å‘½ä»¤**ï¼š
   ```powershell
   New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" -Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force
   ```
3. **é‡å¯è®¡ç®—æœº**
4. **å®‰è£…FlashAttention2**ï¼š
   ```bash
   pip install flash-attn --no-build-isolation
   ```

### æ–¹æ³•3ï¼šä½¿ç”¨conda-forgeï¼ˆå¦‚æœå¯ç”¨ï¼‰

```bash
conda install -c conda-forge flash-attn
```

## ğŸ”§ éªŒè¯å®‰è£…

åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_flash_attn.py`ï¼š

```python
import torch
import flash_attn
from flash_attn import flash_attn_func

print(f"Flash Attentionç‰ˆæœ¬: {flash_attn.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")

# åŠŸèƒ½æµ‹è¯•
if torch.cuda.is_available():
    batch_size, seq_len, n_heads, head_dim = 2, 512, 16, 64
    
    q = torch.randn(batch_size, seq_len, n_heads, head_dim, device='cuda', dtype=torch.float16)
    k = torch.randn(batch_size, seq_len, n_heads, head_dim, device='cuda', dtype=torch.float16)
    v = torch.randn(batch_size, seq_len, n_heads, head_dim, device='cuda', dtype=torch.float16)
    
    # ä½¿ç”¨FlashAttention
    output = flash_attn_func(q, k, v, causal=True)
    print(f"âœ… FlashAttentionæµ‹è¯•æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {output.shape}")
else:
    print("âŒ CUDAä¸å¯ç”¨")
```

## ğŸ“ˆ æ€§èƒ½æå‡

å®‰è£…FlashAttention2åï¼Œæ‚¨å°†è·å¾—ï¼š

- **é€Ÿåº¦æå‡**ï¼šæ¯”æ ‡å‡†attentionå¿«2-10å€
- **å†…å­˜èŠ‚çœ**ï¼šå‡å°‘10-20å€å†…å­˜ä½¿ç”¨
- **æ›´é•¿åºåˆ—**ï¼šæ”¯æŒå¤„ç†æ›´é•¿çš„æ–‡æœ¬åºåˆ—
- **RTX 3060ä¼˜åŒ–**ï¼šå……åˆ†åˆ©ç”¨Ampereæ¶æ„ç‰¹æ€§

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **Windowsé•¿è·¯å¾„é—®é¢˜**ï¼šå¦‚æœé‡åˆ°è·¯å¾„é”™è¯¯ï¼Œå¿…é¡»å¯ç”¨é•¿è·¯å¾„æ”¯æŒå¹¶é‡å¯
2. **ç¼–è¯‘æ—¶é—´**ï¼šä»æºç ç¼–è¯‘å¯èƒ½éœ€è¦30-60åˆ†é’Ÿ
3. **å†…å­˜éœ€æ±‚**ï¼šç¼–è¯‘æ—¶éœ€è¦è‡³å°‘16GB RAM
4. **Visual Studio**ï¼šå¦‚æœä»æºç ç¼–è¯‘ï¼Œéœ€è¦å®‰è£…Visual Studio 2019+

## ğŸ†˜ æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šè·¯å¾„å¤ªé•¿é”™è¯¯
```
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\...\\very_long_path...'
```
**è§£å†³**ï¼šå¯ç”¨Windowsé•¿è·¯å¾„æ”¯æŒï¼ˆè§æ–¹æ³•2ï¼‰

### é—®é¢˜2ï¼šç¼–è¯‘å¤±è´¥
**è§£å†³**ï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„wheelæ–‡ä»¶ï¼ˆæ–¹æ³•1ï¼‰

### é—®é¢˜3ï¼šCUDAç‰ˆæœ¬ä¸åŒ¹é…
**è§£å†³**ï¼šç¡®ä¿PyTorch CUDAç‰ˆæœ¬ä¸ç³»ç»ŸCUDAç‰ˆæœ¬å…¼å®¹

## ğŸ‰ æˆåŠŸå

ä¸€æ—¦æˆåŠŸå®‰è£…FlashAttention2ï¼š

1. **æ›´æ–°embedding_manager.py**ä¸­çš„é…ç½®ï¼Œç§»é™¤ç¦ç”¨FlashAttentionçš„ä»£ç 
2. **è¿è¡ŒGPUæ€§èƒ½æµ‹è¯•**æŸ¥çœ‹æå‡æ•ˆæœ
3. **äº«å—æ›´å¿«çš„æ¨ç†é€Ÿåº¦**ï¼

## ğŸ“š å‚è€ƒèµ„æº

- [Flash Attentionå®˜æ–¹ä»“åº“](https://github.com/Dao-AILab/flash-attention)
- [Windowsé¢„ç¼–è¯‘Wheels](https://github.com/sunsetcoder/flash-attention-windows)
- [Flash Attentionè®ºæ–‡](https://arxiv.org/abs/2205.14135) 